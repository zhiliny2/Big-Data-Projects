{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]\n3.1.3\n"}], "source": "import sys\nprint(sys.version)\nprint(spark.version)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "import pandas as pd\nimport numpy as np\npd.set_option('display.max_colwidth', None)\npd.reset_option('display.max_rows')\nfrom itertools import compress \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(action='ignore')"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}], "source": "import re\nfrom pyspark.ml.feature import MinHashLSH\nfrom pyspark.ml.feature import CountVectorizer,  IDF, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Add \"eagerEval.enabled\" to beautify the way Spark DF is displayed"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"}, {"cell_type": "markdown", "metadata": {}, "source": "**Note:** column \"distCol\" is added to show the distance between each row and the key"}, {"cell_type": "markdown", "metadata": {}, "source": "## Applying LSH to real volume of data"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "100343"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "news_spark = spark.read.parquet('gs://msca-bdp-data-open/news_similar/')\nnews_spark.count()"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "df_text_raw = news_spark.select([\"title\"]).withColumnRenamed('title', 'text')\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>text</th></tr>\n<tr><td>Local police on a...</td></tr>\n<tr><td> Biden Revokes Tr...</td></tr>\n<tr><td>Woolworths superm...</td></tr>\n<tr><td>Boy, 12, dies two...</td></tr>\n<tr><td>TikTok\u00a0Planning t...</td></tr>\n</table>\n", "text/plain": "+--------------------+\n|                text|\n+--------------------+\n|Local police on a...|\n| Biden Revokes Tr...|\n|Woolworths superm...|\n|Boy, 12, dies two...|\n|TikTok\u00a0Planning t...|\n+--------------------+"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "df_text_raw.limit(5)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- text: string (nullable = true)\n\n"}], "source": "df_text_raw.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 1.  Clean the data, remove stopwords and create index"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "text = df_text_raw.rdd.map(lambda x : x['text']).filter(lambda x: x is not None)\n\nStopWords = stopwords.words(\"english\")\n\n# tokens = text\\\n#     .map( lambda document: document.strip().lower())\\\n#     .map( lambda document: re.split(\" \", document))\\\n#     .map( lambda word: [x for x in word if len(x) > 1] )\\\n#     .zipWithIndex()\n\ntokens = text\\\n    .map( lambda document: document.strip().lower())\\\n    .map( lambda document: re.split(\" \", document))\\\n    .map( lambda word: [x for x in word if x.isalnum()])\\\n    .map( lambda word: [x for x in word if x not in StopWords])\\\n    .map( lambda word: [x for x in word if len(x) > 3] )\\\n    .zipWithIndex()"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Exception in thread \"serve RDD 23\" java.net.SocketTimeoutException: Accept timed out\n\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>text</th><th>id</th></tr>\n<tr><td>{Local police on ...</td><td>0</td></tr>\n<tr><td>{ Biden Revokes T...</td><td>1</td></tr>\n<tr><td>{Woolworths super...</td><td>2</td></tr>\n<tr><td>{Boy, 12, dies tw...</td><td>3</td></tr>\n<tr><td>{TikTok\u00a0Planning ...</td><td>4</td></tr>\n</table>\n", "text/plain": "+--------------------+----+\n|                text|  id|\n+--------------------+----+\n|{The Daily Ripple...|9827|\n|{TikTok looks to ...|9828|\n|{How a deepfake T...|9829|\n|{Boy, 5, Who Drov...|9830|\n|{Avril Lavigne De...|9831|\n+--------------------+----+"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "row = Row('text')\ndf_text = text.map(row).zipWithIndex().toDF(['text','id'])\ndf_text.limit(5)"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "100343"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "df_text.count()"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>list_of_words</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[local, police, alert, wake, alleged, school, violence, campaign, reading]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[biden, revokes, chinese, apps, orders, indian, defence, news]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[woolworths, supermarket, worker, bizarre, discovery, viral, tiktok, video, 7news]</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[dies, weeks, attempting, dangerous, tiktok]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[move, business]</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                                                                        list_of_words  \\\n0          [local, police, alert, wake, alleged, school, violence, campaign, reading]   \n1                      [biden, revokes, chinese, apps, orders, indian, defence, news]   \n2  [woolworths, supermarket, worker, bizarre, discovery, viral, tiktok, video, 7news]   \n3                                        [dies, weeks, attempting, dangerous, tiktok]   \n4                                                                    [move, business]   \n\n   id  \n0   0  \n1   1  \n2   2  \n3   3  \n4   4  "}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "df_tokens = spark.createDataFrame(tokens, [\"list_of_words\",'id'])\n\n#Drop records with no tokens\ndf_tokens = df_tokens.where(col('list_of_words').getItem(0).isNotNull())\ndf_tokens.limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- list_of_words: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: long (nullable = true)\n\n"}], "source": "df_tokens.printSchema()"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "98760"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "df_tokens.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 2. Fit countvectorizer to create word features"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "vectorize = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"features\", minDF=1.0)\ndf_vectorize = vectorize.fit(df_tokens).transform(df_tokens)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>list_of_words</th>\n      <th>id</th>\n      <th>features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[local, police, alert, wake, alleged, school, violence, campaign, reading]</td>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[biden, revokes, chinese, apps, orders, indian, defence, news]</td>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[woolworths, supermarket, worker, bizarre, discovery, viral, tiktok, video, 7news]</td>\n      <td>2</td>\n      <td>(1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[dies, weeks, attempting, dangerous, tiktok]</td>\n      <td>3</td>\n      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[move, business]</td>\n      <td>4</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                                                                        list_of_words  \\\n0          [local, police, alert, wake, alleged, school, violence, campaign, reading]   \n1                      [biden, revokes, chinese, apps, orders, indian, defence, news]   \n2  [woolworths, supermarket, worker, bizarre, discovery, viral, tiktok, video, 7news]   \n3                                        [dies, weeks, attempting, dangerous, tiktok]   \n4                                                                    [move, business]   \n\n   id  \\\n0   0   \n1   1   \n2   2   \n3   3   \n4   4   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    features  \n0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)  \n1  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)  \n2  (1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)  \n3  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)  \n4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...)  "}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "df_vectorize.limit(5).toPandas()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 3. Fit MinHashLSH to create hash table\n\n**Note:** Adding more hash tables will increase the accuracy at the expense of training time"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\nmodel = mh.fit(df_vectorize)\ndf_hashed = mh.fit(df_vectorize).transform(df_vectorize).cache()"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>id</th><th>text</th><th>list_of_words</th><th>features</th><th>hashes</th></tr>\n<tr><td>19</td><td>{NJ Twins Ask Ste...</td><td>[twins, adopt, vi...</td><td>(27625,[0,1,4,9,1...</td><td>[[4.7945584E7], [...</td></tr>\n<tr><td>54</td><td>{TikTok testing &#x27;...</td><td>[tiktok, testing,...</td><td>(27625,[0,16,132,...</td><td>[[4.7945584E7], [...</td></tr>\n<tr><td>296</td><td>{Facebook and Tik...</td><td>[facebook, tiktok...</td><td>(27625,[0,21,42,1...</td><td>[[4.7945584E7], [...</td></tr>\n<tr><td>926</td><td>{Biden team asks ...</td><td>[biden, team, ask...</td><td>(27625,[0,67,155,...</td><td>[[4.7945584E7], [...</td></tr>\n<tr><td>965</td><td>{Videos con el ha...</td><td>[videos, hashtag,...</td><td>(27625,[0,2,6],[1...</td><td>[[4.7945584E7], [...</td></tr>\n</table>\n", "text/plain": "+---+--------------------+--------------------+--------------------+--------------------+\n| id|                text|       list_of_words|            features|              hashes|\n+---+--------------------+--------------------+--------------------+--------------------+\n| 19|{NJ Twins Ask Ste...|[twins, adopt, vi...|(27625,[0,1,4,9,1...|[[4.7945584E7], [...|\n| 54|{TikTok testing '...|[tiktok, testing,...|(27625,[0,16,132,...|[[4.7945584E7], [...|\n|296|{Facebook and Tik...|[facebook, tiktok...|(27625,[0,21,42,1...|[[4.7945584E7], [...|\n|926|{Biden team asks ...|[biden, team, ask...|(27625,[0,67,155,...|[[4.7945584E7], [...|\n|965|{Videos con el ha...|[videos, hashtag,...|(27625,[0,2,6],[1...|[[4.7945584E7], [...|\n+---+--------------------+--------------------+--------------------+--------------------+"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "df_hashed_text = df_text.join(df_hashed, \"id\", how = 'left').cache()\ndf_hashed_text.limit(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 4. Establish similarity threshold and return near-duplicate records\n**Note:** we are joining dataframe to itself to get near-duplicate pairs"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Low jaccard distance"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "jaccard_distance = 0.3\n\ndf_dups_text = model.approxSimilarityJoin(df_hashed_text, df_hashed_text, jaccard_distance).filter(\"datasetA.id < datasetB.id\").select(\n            col(\"distCol\"),\n            col(\"datasetA.id\").alias(\"id_A\"),\n            col(\"datasetB.id\").alias(\"id_B\"),\n            col('datasetA.text').alias('text_A'),\n            col('datasetB.text').alias('text_B'),\n#             col('datasetA.list_of_words').alias('words_A'),\n#             col('datasetB.list_of_words').alias('words_B')\n            )"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/11/17 23:30:14 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 11 for reason Container marked as failed: container_1668672144784_0019_01_000012 on host: hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/17 23:30:14 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 12 for reason Container marked as failed: container_1668672144784_0019_01_000013 on host: hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/17 23:30:14 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 12 on hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000013 on host: hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/17 23:30:14 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 11 on hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000012 on host: hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/17 23:32:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 49.0 (TID 944) (hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal executor 18): FetchFailed(BlockManagerId(12, hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=64, mapId=876, reduceId=0, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=986847320000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=12)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=986847320000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=12)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:33:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 49.1 (TID 952) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(11, hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=57, mapId=874, reduceId=0, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=986847320002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=11)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=986847320002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=11)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distCol</th>\n      <th>id_A</th>\n      <th>id_B</th>\n      <th>text_A</th>\n      <th>text_B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>418</td>\n      <td>71579</td>\n      <td>(#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)</td>\n      <td>(#\u81ea\u5df1\u7684\u6b4c\u81ea\u5df1\u505a Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>418</td>\n      <td>12303</td>\n      <td>(#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)</td>\n      <td>(#votetiktop Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>418</td>\n      <td>69136</td>\n      <td>(#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)</td>\n      <td>(#happysweat Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>418</td>\n      <td>444</td>\n      <td>(#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)</td>\n      <td>(#joseywales Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>418</td>\n      <td>6163</td>\n      <td>(#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)</td>\n      <td>(#rangamma Hashtag Videos on TikTok,)</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   distCol  id_A   id_B                                    text_A  \\\n0      0.0   418  71579  (#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)   \n1      0.0   418  12303  (#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)   \n2      0.0   418  69136  (#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)   \n3      0.0   418    444  (#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)   \n4      0.0   418   6163  (#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Hashtag Videos on TikTok,)   \n\n                                    text_B  \n0     (#\u81ea\u5df1\u7684\u6b4c\u81ea\u5df1\u505a Hashtag Videos on TikTok,)  \n1  (#votetiktop Hashtag Videos on TikTok,)  \n2  (#happysweat Hashtag Videos on TikTok,)  \n3  (#joseywales Hashtag Videos on TikTok,)  \n4    (#rangamma Hashtag Videos on TikTok,)  "}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "df_dups_30 = df_dups_text\ndf_dups_text.cache()\ndf_dups_text.limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/11/17 23:37:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 60.0 (TID 1174) (hub-msca-bdp-dphub-students-zhiliny-sw-zqlv.c.msca-bdp-students.internal executor 5): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=12, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 60.0 (TID 1167) (hub-msca-bdp-dphub-students-zhiliny-sw-zqlv.c.msca-bdp-students.internal executor 5): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=5, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 60.0 (TID 1171) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=9, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 60.0 (TID 1164) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=2, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 60.0 (TID 1170) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 10): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=8, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 60.0 (TID 1163) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 10): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=1, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 60.0 (TID 1172) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 6): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=10, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 60.0 (TID 1165) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 6): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=3, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 60.0 (TID 1173) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 4): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=11, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.0 in stage 60.0 (TID 1166) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 4): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=4, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.0 in stage 60.0 (TID 1168) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 7): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=6, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.0 in stage 60.0 (TID 1176) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 7): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=14, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 60.0 (TID 1169) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 9): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=7, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.0 in stage 60.0 (TID 1175) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 9): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=13, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 16.0 in stage 60.0 (TID 1178) (hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal executor 27): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=16, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.0 in stage 60.0 (TID 1177) (hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal executor 27): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=15, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:35 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 18.0 in stage 60.0 (TID 1180) (hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal executor 28): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=18, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:35 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 17.0 in stage 60.0 (TID 1179) (hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal executor 28): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=17, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 27.0 in stage 60.0 (TID 1189) (hub-msca-bdp-dphub-students-zhiliny-sw-zqlv.c.msca-bdp-students.internal executor 5): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=27, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.0 in stage 60.0 (TID 1182) (hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal executor 29): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=20, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401003,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401003,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 19.0 in stage 60.0 (TID 1181) (hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal executor 29): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=19, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 23.0 in stage 60.0 (TID 1185) (hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal executor 32): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=23, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401007,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401007,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 24.0 in stage 60.0 (TID 1186) (hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal executor 32): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=24, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401006,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401006,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 26.0 in stage 60.0 (TID 1188) (hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal executor 30): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=26, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401009,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401009,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=21)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 25.0 in stage 60.0 (TID 1187) (hub-msca-bdp-dphub-students-zhiliny-sw-z1x2.c.msca-bdp-students.internal executor 30): FetchFailed(BlockManagerId(22, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=47, mapId=867, reduceId=25, message=\norg.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=853564401010,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=853564401010,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1668672144784_0019, execId=22)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/17 23:37:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 22.0 in stage 60.0 (TID 1184) (hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal executor 31): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=22, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)\n\n)\n22/11/17 23:37:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 21.0 in stage 60.0 (TID 1183) (hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal executor 31): FetchFailed(BlockManagerId(21, hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=126, mapId=909, reduceId=21, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-9x8c.c.msca-bdp-students.internal/10.128.0.2:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n[Stage 65:====================================================>   (27 + 2) / 29]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total records:  100343\nDuplicate titles based on { 0.3 } jaccard distance:  41627\nUnique titles based on { 0.3 } jaccard distance:  0.3 :  58716\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "records = df_hashed_text.count()\ndups = df_dups_text.select('id_A').distinct().count()\nuniques = records - dups\n\nprint ('Total records: ', records)\nprint ('Duplicate titles based on {', jaccard_distance, '} jaccard distance: ', dups)\nprint ('Unique titles based on {', jaccard_distance, '} jaccard distance: ', jaccard_distance, ': ', uniques)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 640x480 with 1 Axes>"}, "metadata": {}, "output_type": "display_data"}], "source": "dups_df = pd.DataFrame.from_dict({'near_dups': [dups], 'unique': [uniques]})\n\nax=dups_df.plot(kind = 'bar',y=['near_dups', 'unique'], fontsize=10, color=['C0', 'C1'], align='center', width=0.8, xlabel=\"Duplicates vs. Unique\")\nax.set_title('News title duplication analysis', fontsize=15)\nfor p in ax.patches:\n       ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()/2), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points') "}, {"cell_type": "markdown", "metadata": {}, "source": "#### Medium jaccard distance"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "jaccard_distance = 0.5\n\ndf_dups_text = model.approxSimilarityJoin(df_hashed_text, df_hashed_text, jaccard_distance).filter(\"datasetA.id < datasetB.id\").select(\n            col(\"distCol\"),\n            col(\"datasetA.id\").alias(\"id_A\"),\n            col(\"datasetB.id\").alias(\"id_B\"),\n            col('datasetA.text').alias('text_A'),\n            col('datasetB.text').alias('text_B'),\n#             col('datasetA.list_of_words').alias('words_A'),\n#             col('datasetB.list_of_words').alias('words_B')\n            )    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distCol</th>\n      <th>id_A</th>\n      <th>id_B</th>\n      <th>text_A</th>\n      <th>text_B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.25</td>\n      <td>2568</td>\n      <td>21835</td>\n      <td>(Netflix, TikTok block services in Russia to avoid crackdown - AP - Setopati,)</td>\n      <td>(Netflix, TikTok block services in Russia to avoid crackdown - WTMJ,)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00</td>\n      <td>5628</td>\n      <td>23673</td>\n      <td>(#knowdown Hashtag Videos on TikTok,)</td>\n      <td>(#poltagon Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>5628</td>\n      <td>13886</td>\n      <td>(#knowdown Hashtag Videos on TikTok,)</td>\n      <td>(#ronniecolemanpolice Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00</td>\n      <td>5628</td>\n      <td>33284</td>\n      <td>(#knowdown Hashtag Videos on TikTok,)</td>\n      <td>(#\u53ef\u5904\u5173\u7cfb\u4e0d\u5904\u95fa\u871c\u2728 Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00</td>\n      <td>5628</td>\n      <td>19074</td>\n      <td>(#knowdown Hashtag Videos on TikTok,)</td>\n      <td>(#uditnarayanan Hashtag Videos on TikTok,)</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   distCol  id_A   id_B  \\\n0     0.25  2568  21835   \n1     0.00  5628  23673   \n2     0.00  5628  13886   \n3     0.00  5628  33284   \n4     0.00  5628  19074   \n\n                                                                           text_A  \\\n0  (Netflix, TikTok block services in Russia to avoid crackdown - AP - Setopati,)   \n1                                           (#knowdown Hashtag Videos on TikTok,)   \n2                                           (#knowdown Hashtag Videos on TikTok,)   \n3                                           (#knowdown Hashtag Videos on TikTok,)   \n4                                           (#knowdown Hashtag Videos on TikTok,)   \n\n                                                                  text_B  \n0  (Netflix, TikTok block services in Russia to avoid crackdown - WTMJ,)  \n1                                  (#poltagon Hashtag Videos on TikTok,)  \n2                       (#ronniecolemanpolice Hashtag Videos on TikTok,)  \n3                                 (#\u53ef\u5904\u5173\u7cfb\u4e0d\u5904\u95fa\u871c\u2728 Hashtag Videos on TikTok,)  \n4                             (#uditnarayanan Hashtag Videos on TikTok,)  "}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "df_dups_50 = df_dups_text\ndf_dups_text.cache()\ndf_dups_text.limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 89:=====================================================>(197 + 3) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total records:  100343\nDuplicate titles based on { 0.5 } jaccard distance:  49379\nUnique titles based on { 0.5 } jaccard distance:  0.5 :  50964\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "records = df_hashed_text.count()\ndups = df_dups_text.select('id_A').distinct().count()\nuniques = records - dups\n\nprint ('Total records: ', records)\nprint ('Duplicate titles based on {', jaccard_distance, '} jaccard distance: ', dups)\nprint ('Unique titles based on {', jaccard_distance, '} jaccard distance: ', jaccard_distance, ': ', uniques)"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 640x480 with 1 Axes>"}, "metadata": {}, "output_type": "display_data"}], "source": "dups_df = pd.DataFrame.from_dict({'near_dups': [dups], 'unique': [uniques]})\n\nax=dups_df.plot(kind = 'bar',y=['near_dups', 'unique'], fontsize=10, color=['C0', 'C1'], align='center', width=0.8, xlabel=\"Duplicates vs. Unique\")\nax.set_title('News title duplication analysis', fontsize=15)\nfor p in ax.patches:\n       ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()/2), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points') "}, {"cell_type": "markdown", "metadata": {}, "source": "#### High jaccard distance"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": "jaccard_distance = 0.7\n\ndf_dups_text = model.approxSimilarityJoin(df_hashed_text, df_hashed_text, jaccard_distance).filter(\"datasetA.id < datasetB.id\").select(\n            col(\"distCol\"),\n            col(\"datasetA.id\").alias(\"id_A\"),\n            col(\"datasetB.id\").alias(\"id_B\"),\n            col('datasetA.text').alias('text_A'),\n            col('datasetB.text').alias('text_B'),\n#             col('datasetA.list_of_words').alias('words_A'),\n#             col('datasetB.list_of_words').alias('words_B')\n            )    "}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/11/18 00:35:20 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 47 for reason Container marked as failed: container_1668672144784_0019_01_000051 on host: hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:35:20 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 48 for reason Container marked as failed: container_1668672144784_0019_01_000052 on host: hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:35:20 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 48 on hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000052 on host: hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:35:20 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 47 on hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000051 on host: hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distCol</th>\n      <th>id_A</th>\n      <th>id_B</th>\n      <th>text_A</th>\n      <th>text_B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>1480</td>\n      <td>33312</td>\n      <td>(#waistlinewednesdays Hashtag Videos on TikTok,)</td>\n      <td>(#thelablove_feature Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>1480</td>\n      <td>40998</td>\n      <td>(#waistlinewednesdays Hashtag Videos on TikTok,)</td>\n      <td>(#hurstville Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>1480</td>\n      <td>51016</td>\n      <td>(#waistlinewednesdays Hashtag Videos on TikTok,)</td>\n      <td>(#_prfect Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>1480</td>\n      <td>39953</td>\n      <td>(#waistlinewednesdays Hashtag Videos on TikTok,)</td>\n      <td>(#leiaparaseubebe Hashtag Videos on TikTok,)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.666667</td>\n      <td>1480</td>\n      <td>95142</td>\n      <td>(#waistlinewednesdays Hashtag Videos on TikTok,)</td>\n      <td>(#dahoodtips TikTok \u092a\u0930 \u0939\u0948\u0936\u091f\u0948\u0917 \u0935\u0940\u0921\u093f\u092f\u094b,)</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "    distCol  id_A   id_B                                            text_A  \\\n0  0.000000  1480  33312  (#waistlinewednesdays Hashtag Videos on TikTok,)   \n1  0.000000  1480  40998  (#waistlinewednesdays Hashtag Videos on TikTok,)   \n2  0.000000  1480  51016  (#waistlinewednesdays Hashtag Videos on TikTok,)   \n3  0.000000  1480  39953  (#waistlinewednesdays Hashtag Videos on TikTok,)   \n4  0.666667  1480  95142  (#waistlinewednesdays Hashtag Videos on TikTok,)   \n\n                                            text_B  \n0  (#thelablove_feature Hashtag Videos on TikTok,)  \n1          (#hurstville Hashtag Videos on TikTok,)  \n2             (#_prfect Hashtag Videos on TikTok,)  \n3     (#leiaparaseubebe Hashtag Videos on TikTok,)  \n4           (#dahoodtips TikTok \u092a\u0930 \u0939\u0948\u0936\u091f\u0948\u0917 \u0935\u0940\u0921\u093f\u092f\u094b,)  "}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": "df_dups_70 = df_dups_text\ndf_dups_text.cache()\ndf_dups_text.limit(5).toPandas()"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 125:====================================================>(198 + 2) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total records:  100343\nDuplicate titles based on { 0.7 } jaccard distance:  76971\nUnique titles based on { 0.7 } jaccard distance:  0.7 :  23372\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "records = df_hashed_text.count()\ndups = df_dups_text.select('id_A').distinct().count()\nuniques = records - dups\n\nprint ('Total records: ', records)\nprint ('Duplicate titles based on {', jaccard_distance, '} jaccard distance: ', dups)\nprint ('Unique titles based on {', jaccard_distance, '} jaccard distance: ', jaccard_distance, ': ', uniques)"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 640x480 with 1 Axes>"}, "metadata": {}, "output_type": "display_data"}], "source": "dups_df = pd.DataFrame.from_dict({'near_dups': [dups], 'unique': [uniques]})\n\nax=dups_df.plot(kind = 'bar',y=['near_dups', 'unique'], fontsize=10, color=['C0', 'C1'], align='center', width=0.8, xlabel=\"Duplicates vs. Unique\")\nax.set_title('News title duplication analysis', fontsize=15)\nfor p in ax.patches:\n       ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()/2), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points') "}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 5. Compare thresholds side-by-side"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Select all pairs of duplicate titles across jaccard similarities"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 149.0 in stage 140.0 (TID 3618) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 59): FetchFailed(BlockManagerId(18, hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=76, mapId=882, reduceId=149, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 174.0 in stage 140.0 (TID 3619) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(24, hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=146, mapId=917, reduceId=174, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 105.0 in stage 140.0 (TID 3611) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 55): FetchFailed(BlockManagerId(26, hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=173, mapId=928, reduceId=105, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 106.0 in stage 140.0 (TID 3612) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 56): FetchFailed(BlockManagerId(24, hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=146, mapId=917, reduceId=106, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 126.0 in stage 140.0 (TID 3614) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 51): FetchFailed(BlockManagerId(19, hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=109, mapId=899, reduceId=126, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 138.0 in stage 140.0 (TID 3615) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 45): FetchFailed(BlockManagerId(23, hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=143, mapId=915, reduceId=138, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-7q4x.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 139.0 in stage 140.0 (TID 3616) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 46): FetchFailed(BlockManagerId(26, hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=173, mapId=928, reduceId=139, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 148.0 in stage 140.0 (TID 3617) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 57): FetchFailed(BlockManagerId(14, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=91, mapId=890, reduceId=148, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 125.0 in stage 140.0 (TID 3613) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 61): FetchFailed(BlockManagerId(19, hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=109, mapId=899, reduceId=125, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 175.0 in stage 140.0 (TID 3620) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 52): FetchFailed(BlockManagerId(18, hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=76, mapId=882, reduceId=175, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 140.1 (TID 3830) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(25, hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=176, mapId=930, reduceId=139, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 140.1 (TID 3825) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 60): FetchFailed(BlockManagerId(16, hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=56, mapId=873, reduceId=105, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 140.1 (TID 3832) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 52): FetchFailed(BlockManagerId(20, hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=104, mapId=848, reduceId=149, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 140.1 (TID 3827) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 4): FetchFailed(BlockManagerId(17, hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=89, mapId=889, reduceId=125, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 140.1 (TID 3828) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 46): FetchFailed(BlockManagerId(15, hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=53, mapId=871, reduceId=126, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-hnq2.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 140.1 (TID 3826) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 54): FetchFailed(BlockManagerId(25, hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=176, mapId=930, reduceId=106, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 140.1 (TID 3833) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(25, hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=176, mapId=930, reduceId=174, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-nrx4.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 140.1 (TID 3834) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 57): FetchFailed(BlockManagerId(17, hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=89, mapId=889, reduceId=175, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-45jd.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.0 in stage 140.1 (TID 3831) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 51): FetchFailed(BlockManagerId(20, hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=104, mapId=848, reduceId=148, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:43:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.0 in stage 140.1 (TID 3829) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 50): FetchFailed(BlockManagerId(20, hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=104, mapId=848, reduceId=138, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-dc3f.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 140.2 (TID 3874) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 46): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=174, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.0 in stage 140.2 (TID 3872) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 51): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=148, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 140.2 (TID 3871) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 50): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=139, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 140.2 (TID 3866) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 54): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=105, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 140.2 (TID 3869) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 61): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=126, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 140.2 (TID 3868) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 56): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=125, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 140.2 (TID 3873) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 59): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=149, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.0 in stage 140.2 (TID 3870) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=138, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 140.2 (TID 3875) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 57): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=175, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:45:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 140.2 (TID 3867) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 10): FetchFailed(BlockManagerId(13, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=10, mapIndex=82, mapId=885, reduceId=106, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "14440844\n"}, {"name": "stderr", "output_type": "stream", "text": "22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 91.0 in stage 149.0 (TID 4006) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=91, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 93.0 in stage 149.0 (TID 4008) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 46): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=93, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 149.0 (TID 3980) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 55): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=2, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 95.0 in stage 149.0 (TID 4010) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 59): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=95, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 50.0 in stage 149.0 (TID 3994) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 59): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=50, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 47.0 in stage 149.0 (TID 3991) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 54): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=47, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 94.0 in stage 149.0 (TID 4009) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 51): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=94, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 149.0 (TID 3983) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 60): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=12, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 149.0 (TID 3981) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 57): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=8, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 65.0 in stage 149.0 (TID 3997) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 57): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=65, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 66.0 in stage 149.0 (TID 3998) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 45): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=66, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 149.0 (TID 3982) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 45): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=11, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 38.0 in stage 149.0 (TID 3989) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 61): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=38, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 39.0 in stage 149.0 (TID 3990) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=39, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 23.0 in stage 149.0 (TID 3988) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 52): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=23, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 55.0 in stage 149.0 (TID 3996) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 55): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=55, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 82.0 in stage 149.0 (TID 4004) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 52): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=82, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 19.0 in stage 149.0 (TID 3986) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 56): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=19, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 92.0 in stage 149.0 (TID 4007) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 54): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=92, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 49.0 in stage 149.0 (TID 3993) (hub-msca-bdp-dphub-students-zhiliny-sw-djrn.c.msca-bdp-students.internal executor 51): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=49, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 90.0 in stage 149.0 (TID 4005) (hub-msca-bdp-dphub-students-zhiliny-sw-x9l6.c.msca-bdp-students.internal executor 61): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=90, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 73.0 in stage 149.0 (TID 4002) (hub-msca-bdp-dphub-students-zhiliny-sw-8q3f.c.msca-bdp-students.internal executor 56): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=73, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 48.0 in stage 149.0 (TID 3992) (hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal executor 46): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=48, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 149.0 (TID 3979) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 50): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=1, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 68.0 in stage 149.0 (TID 3999) (hub-msca-bdp-dphub-students-zhiliny-sw-2p0v.c.msca-bdp-students.internal executor 60): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=68, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 22.0 in stage 149.0 (TID 3987) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 49): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=22, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 79.0 in stage 149.0 (TID 4003) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 49): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=79, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 53.0 in stage 149.0 (TID 3995) (hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal executor 50): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=53, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.0 in stage 149.0 (TID 3985) (hub-msca-bdp-dphub-students-zhiliny-sw-24hj.c.msca-bdp-students.internal executor 62): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=14, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 71.0 in stage 149.0 (TID 4001) (hub-msca-bdp-dphub-students-zhiliny-sw-24hj.c.msca-bdp-students.internal executor 62): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=71, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 103.0 in stage 149.0 (TID 4120) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 9): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=103, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 114.0 in stage 149.0 (TID 4129) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 9): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=114, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 116.0 in stage 149.0 (TID 4130) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 7): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=116, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 149.0 (TID 4118) (hub-msca-bdp-dphub-students-zhiliny-w-1.c.msca-bdp-students.internal executor 7): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=98, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 70.0 in stage 149.0 (TID 4000) (hub-msca-bdp-dphub-students-zhiliny-sw-24hj.c.msca-bdp-students.internal executor 63): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=70, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 109.0 in stage 149.0 (TID 4125) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 6): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=109, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 134.0 in stage 149.0 (TID 4139) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 6): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=134, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 126.0 in stage 149.0 (TID 4138) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 4): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=126, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 124.0 in stage 149.0 (TID 4137) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 4): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=124, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 122.0 in stage 149.0 (TID 4136) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 10): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=122, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 138.0 in stage 149.0 (TID 4142) (hub-msca-bdp-dphub-students-zhiliny-sw-ztc5.c.msca-bdp-students.internal executor 10): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=138, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 120.0 in stage 149.0 (TID 4133) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=120, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 117.0 in stage 149.0 (TID 4131) (hub-msca-bdp-dphub-students-zhiliny-w-0.c.msca-bdp-students.internal executor 8): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=117, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 143.0 in stage 149.0 (TID 4144) (hub-msca-bdp-dphub-students-zhiliny-sw-zqlv.c.msca-bdp-students.internal executor 5): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=143, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 139.0 in stage 149.0 (TID 4143) (hub-msca-bdp-dphub-students-zhiliny-sw-zqlv.c.msca-bdp-students.internal executor 5): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=139, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n22/11/18 00:48:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.0 in stage 149.0 (TID 3984) (hub-msca-bdp-dphub-students-zhiliny-sw-24hj.c.msca-bdp-students.internal executor 63): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=13, message=\norg.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\n)\n22/11/18 00:48:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 144.0 in stage 149.0 (TID 4145) (hub-msca-bdp-dphub-students-zhiliny-sw-7xs5.c.msca-bdp-students.internal executor 53): FetchFailed(BlockManagerId(39, hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal, 7337, None), shuffleId=14, mapIndex=19, mapId=1644, reduceId=144, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1425)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1352)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1416)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1239)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-zhiliny-sw-rk9r.c.msca-bdp-students.internal\n\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1136)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1064)\n\tat java.net.InetAddress.getByName(InetAddress.java:1014)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t... 2 more\n\n)\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "14645154\n24772685\n"}], "source": "df_dups_30_id = df_dups_30.select(col('id_A').alias('id_30_A'), col('id_B').alias('id_30_B'))\ndf_dups_50_id = df_dups_50.select(col('id_A').alias('id_50_A'), col('id_B').alias('id_50_B'))\ndf_dups_70_id = df_dups_70.select(col('text_A'), col('text_B'),\\\n                                  col('id_A').alias('id_70_A'), col('id_B').alias('id_70_B'))\n\nprint(df_dups_30_id.count())\nprint(df_dups_50_id.count())\nprint(df_dups_70_id.count())"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                ]]\r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>text_A</th><th>text_B</th><th>id_70_A</th><th>id_70_B</th><th>id_50_A</th><th>id_50_B</th><th>id_30_A</th><th>id_30_B</th></tr>\n<tr><td>{TikTok may let c...</td><td>{#\u310e\u3127\u3124\u7cfb\u5217 Hashtag V...</td><td>26</td><td>76971</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n<tr><td>{TikTok may let c...</td><td>{#desi_sc Hashtag...</td><td>26</td><td>92249</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n<tr><td>{US drops Trump o...</td><td>{US drops Trump o...</td><td>270</td><td>20185</td><td>270</td><td>20185</td><td>270</td><td>20185</td></tr>\n<tr><td>{#\u0e2a\u0e32\u0e27\u0e1a\u0e35\u0e17\u0e32\u0e40\u0e01\u0e49\u0e19 Has...</td><td>{#bodyluv Hashtag...</td><td>418</td><td>98903</td><td>418</td><td>98903</td><td>418</td><td>98903</td></tr>\n<tr><td>{#lennischallenge...</td><td>{#haneunjung Hash...</td><td>446</td><td>54005</td><td>446</td><td>54005</td><td>446</td><td>54005</td></tr>\n</table>\n", "text/plain": "+--------------------+--------------------+-------+-------+-------+-------+-------+-------+\n|              text_A|              text_B|id_70_A|id_70_B|id_50_A|id_50_B|id_30_A|id_30_B|\n+--------------------+--------------------+-------+-------+-------+-------+-------+-------+\n|{How to buy TikTo...|{How to add and s...|    167|  80645|   null|   null|   null|   null|\n|{TikToker claims ...|{TikToker claims ...|    191|  79586|    191|  79586|    191|  79586|\n|{How to See Who V...|{#sraik TikTok \u092a\u0930...|    264|  70644|   null|   null|   null|   null|\n|{Gamelancer enter...|{Gamelancer enter...|    288|  43523|    288|  43523|    288|  43523|\n|{Report: TikTok s...|{Report: TikTok s...|    367|  18484|    367|  18484|    367|  18484|\n+--------------------+--------------------+-------+-------+-------+-------+-------+-------+"}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": "dups_combined = df_dups_70_id.join(df_dups_50_id, on=((df_dups_70_id.id_70_A == df_dups_50_id.id_50_A) & (df_dups_70_id.id_70_B == df_dups_50_id.id_50_B)), how=\"left_outer\")\\\n.join(df_dups_30_id, on=((df_dups_70_id.id_70_A == df_dups_30_id.id_30_A) & (df_dups_70_id.id_70_B == df_dups_30_id.id_30_B)), how=\"left_outer\")\n\ndups_combined.limit(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Create a Duplicate vs Non-Dup flag for each jaccard similarity"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                0]\r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>text_A</th><th>text_B</th><th>threshold_30</th><th>threshold_50</th><th>threshold_70</th></tr>\n<tr><td>{Woolworths super...</td><td>{ALDI supermarket...</td><td>Non-Dup</td><td>Non-Dup</td><td>Duplicate</td></tr>\n<tr><td>{Woolworths super...</td><td>{Woolworths super...</td><td>Non-Dup</td><td>Non-Dup</td><td>Duplicate</td></tr>\n<tr><td>{Boy, 12, dies tw...</td><td>{Parents sue TikT...</td><td>Non-Dup</td><td>Non-Dup</td><td>Duplicate</td></tr>\n<tr><td>{Boy, 12, dies tw...</td><td>{Okla. boy, 12, d...</td><td>Non-Dup</td><td>Non-Dup</td><td>Duplicate</td></tr>\n<tr><td>{Boy, 12, dies tw...</td><td>{Tijuana boy, 9, ...</td><td>Non-Dup</td><td>Non-Dup</td><td>Duplicate</td></tr>\n</table>\n", "text/plain": "+--------------------+--------------------+------------+------------+------------+\n|              text_A|              text_B|threshold_30|threshold_50|threshold_70|\n+--------------------+--------------------+------------+------------+------------+\n|{Woolworths super...|{7-Eleven Florida...|     Non-Dup|     Non-Dup|   Duplicate|\n|{Woolworths super...|{Woolworths super...|     Non-Dup|     Non-Dup|   Duplicate|\n|{Boy, 12, dies tw...|{Temple girl dies...|     Non-Dup|     Non-Dup|   Duplicate|\n|{Wendy Williams\u2019 ...|{Protests Over De...|     Non-Dup|     Non-Dup|   Duplicate|\n|{     tiktok: Kia...|{     TalkShopLiv...|     Non-Dup|     Non-Dup|   Duplicate|\n+--------------------+--------------------+------------+------------+------------+"}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": "dups_combined = dups_combined.\\\nwithColumn(\"threshold_30\",\\\n           when(col(\"id_30_A\").isNotNull(), \"Duplicate\").\n           otherwise(\"Non-Dup\")).\\\nwithColumn(\"threshold_50\",\\\n           when(col(\"id_50_A\").isNotNull(), \"Duplicate\").\n           otherwise(\"Non-Dup\")).\\\nwithColumn(\"threshold_70\",\\\n           when(col(\"id_70_A\").isNotNull(), \"Duplicate\").\n           otherwise(\"Non-Dup\")).\\\ndrop('id_30_A', 'id_30_B', 'id_50_A', 'id_50_B', 'id_70_A', 'id_70_B')\n\ndups_combined.limit(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Select a sample of records across 3 thresholds (jaccard similarities):\n* Records that are marked as duplicate by all three thresholds (meet the level of **threshold_30**)\n* Records that are identified as non-duplicate by **threshold_30**, while being marked as dups by both **threshold_50** and **threshold_70**\n* Records that are identified as non-duplicate by both **threshold_30** and **threshold_50**, while being marked as dups by **threshold_70**"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": "dups_combined_sample = \\\ndups_combined.filter((\"threshold_30 == 'Duplicate'\")).sample(False, 0.1, 12345).limit(10).\\\nunion\\\n(dups_combined.filter((\"threshold_30 == 'Non-Dup' and threshold_50 == 'Duplicate'\")).sample(False, 0.1, 12345).limit(10)).\\\nunion\\\n(dups_combined.filter((\"threshold_50 == 'Non-Dup'\")).sample(False, 0.1, 12345).limit(10)).\\\norderBy('threshold_30', 'threshold_50', 'threshold_70')"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                00]\r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_A</th>\n      <th>text_B</th>\n      <th>threshold_30</th>\n      <th>threshold_50</th>\n      <th>threshold_70</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Coldplay to perform livestreamed set exclusively on TikTok,)</td>\n      <td>(Coldplay to perform livestreamed set exclusively on TikTok,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(Trump's attempt to ban TikTok in US dropped by Biden White House | 10tv.com,)</td>\n      <td>(Trump's attempt to ban TikTok in US dropped by Biden White House | wzzm13.com,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(Dutch group launches data harvesting claim against TikTok ,)</td>\n      <td>(Dutch group launches data harvesting claim against TikTok,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Founder of TikTok's Chinese owner stepping down as CEO,)</td>\n      <td>(Founder of TikTok's Chinese owner stepping down as CEO,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(YouTube, Facebook, TikTok block Russian state media across Europe,)</td>\n      <td>(Meta, YouTube, and TikTok block Russian state media in Europe : worldnews,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(TikTok star\u2019s son killed in Alabama shooting, family confirms | KTSM 9 News,)</td>\n      <td>(TikTok star\u2019s son killed in Alabama shooting, family confirms | KETK.com | FOX51.com,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(Charge: Teen hit disabled teacher; apparent TikTok challenge,)</td>\n      <td>(Charge: Teen hit disabled teacher; apparent TikTok challenge - NewsBreak,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(US drops Trump order targeting TikTok, plans its own review,)</td>\n      <td>(US drops Trump order targeting TikTok, plans its own review | WTOP,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(North Carolina tattoo artist amasses large TikTok following,)</td>\n      <td>(North Carolina tattoo artist amasses large TikTok following | Biloxi Sun Herald,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(Florida Dollar General store manager says she was fired over viral TikToks | mypanhandle.com,)</td>\n      <td>(Florida Dollar General store manager says she was fired over viral TikToks | OurQuadCities,)</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(Netflix, TikTok block services in Russia amid media crackdown,)</td>\n      <td>(Netflix, TikTok block services in Russia to avoid crackdown,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(What the heck is Pink Sauce and why should anyone not on TikTok care? Let us explain - The Golden Star,)</td>\n      <td>(What the heck is Pink Sauce and why should anyone not on TikTok care? Let us explain - Kimberley Daily Bulletin,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(TikTok (\u30c6\u30a3\u30c3\u30af\u30c8\u30c3\u30af) \u3067 Naopaka\ud83c\uddef\ud83c\uddf5 (@paka_1310) \u306e\u52d5\u753b\u3092\u898b\u308b ,)</td>\n      <td>(TikTok (\u30c6\u30a3\u30c3\u30af\u30c8\u30c3\u30af) \u3067 \u7dba\u9e97\u306a\u666f\u8272 (@tomo_pi) \u306e\u52d5\u753b\u3092\u898b\u308b ,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(FCC commissioner demands Apple and Google remove TikTok app | Tom's Guide,)</td>\n      <td>(FCC commissioner calls on Apple and Google to remove TikTok from their app stores,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(How China's TikTok, Facebook influencers push propaganda | myMotherLode.com,)</td>\n      <td>(How China's TikTok, Facebook influencers push propaganda | Daily Mail Online,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(How China's TikTok, Facebook influencers push propaganda - NZ Herald,)</td>\n      <td>(How China\u2019s TikTok, Facebook Influencers Push Propaganda \u2013 NBC Los Angeles,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WSPA 7NEWS,)</td>\n      <td>(\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WDVM25 &amp; DCW50 | Washington, DC,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(Mum\u2019s stark warning over \u2018vile\u2019 graphic TikTok video being shared online | Yorkshire Post,)</td>\n      <td>(Mum\u2019s stark warning over \u2018vile\u2019 graphic TikTok video being shared online | Hucknall Dispatch,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(1 arrested in shooting that wounded TikTok star, killed teen \u2013 CBS17.com,)</td>\n      <td>(1 arrested in shooting that wounded TikTok star, killed teen \u2013 Boston News, Weather, Sports | WHDH 7News,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(The Sounds of TikTok - News Break,)</td>\n      <td>(Idaho\u2019s TikTok Cop Has Been Fired - News Break,)</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>(Boy, 12, dies two weeks after attempting dangerous \u2018Blackout Challenge\u2019 on TikTok | 7NEWS.com.au,)</td>\n      <td>(Okla. boy, 12, dies attempting TikTok challenge, police say,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>(Selena Gomez and Francia meet again in a TikTok video - Code List,)</td>\n      <td>(Tom Cruise is mistaken for a Japanese man for a viral video on TikTok - Code List,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>(TikTok star Hareem Shah's new video goes viral,)</td>\n      <td>(\"Pride cake\" TikTok Goes Viral Celebrating Mom's Allyship,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>(TikTok star Hareem Shah's new video goes viral,)</td>\n      <td>('Deepfake' Tom Cruise goes viral on TikTok with - One News Page VIDEO,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>(TikTok star Hareem Shah's new video goes viral,)</td>\n      <td>(\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WKBN.com,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>(TikTok may let creators use their own product links in videos,)</td>\n      <td>(#\u5353\u5b9d\u5973\u795e Hashtag Videos on TikTok,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>(TikTok may let creators use their own product links in videos,)</td>\n      <td>(#\u0e2d\u0e34\u0e42\u0e19\u0e23\u0e34 Hashtag Videos on TikTok,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>(TikTok may let creators use their own product links in videos,)</td>\n      <td>(#life_style_mekhma Hashtag Videos on TikTok,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>(TikTok may let creators use their own product links in videos,)</td>\n      <td>(#backgroundnoise Hashtag Videos on TikTok,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>(TikTok may let creators use their own product links in videos,)</td>\n      <td>(#freesa Hashtag Videos on TikTok,)</td>\n      <td>Non-Dup</td>\n      <td>Non-Dup</td>\n      <td>Duplicate</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                                                                                                       text_A  \\\n0                                               (Coldplay to perform livestreamed set exclusively on TikTok,)   \n1                              (Trump's attempt to ban TikTok in US dropped by Biden White House | 10tv.com,)   \n2                                               (Dutch group launches data harvesting claim against TikTok ,)   \n3                                                   (Founder of TikTok's Chinese owner stepping down as CEO,)   \n4                                        (YouTube, Facebook, TikTok block Russian state media across Europe,)   \n5                              (TikTok star\u2019s son killed in Alabama shooting, family confirms | KTSM 9 News,)   \n6                                             (Charge: Teen hit disabled teacher; apparent TikTok challenge,)   \n7                                              (US drops Trump order targeting TikTok, plans its own review,)   \n8                                              (North Carolina tattoo artist amasses large TikTok following,)   \n9             (Florida Dollar General store manager says she was fired over viral TikToks | mypanhandle.com,)   \n10                                           (Netflix, TikTok block services in Russia amid media crackdown,)   \n11  (What the heck is Pink Sauce and why should anyone not on TikTok care? Let us explain - The Golden Star,)   \n12                                                       (TikTok (\u30c6\u30a3\u30c3\u30af\u30c8\u30c3\u30af) \u3067 Naopaka\ud83c\uddef\ud83c\uddf5 (@paka_1310) \u306e\u52d5\u753b\u3092\u898b\u308b ,)   \n13                               (FCC commissioner demands Apple and Google remove TikTok app | Tom's Guide,)   \n14                             (How China's TikTok, Facebook influencers push propaganda | myMotherLode.com,)   \n15                                    (How China's TikTok, Facebook influencers push propaganda - NZ Herald,)   \n16                       (\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WSPA 7NEWS,)   \n17               (Mum\u2019s stark warning over \u2018vile\u2019 graphic TikTok video being shared online | Yorkshire Post,)   \n18                                (1 arrested in shooting that wounded TikTok star, killed teen \u2013 CBS17.com,)   \n19                                                                       (The Sounds of TikTok - News Break,)   \n20        (Boy, 12, dies two weeks after attempting dangerous \u2018Blackout Challenge\u2019 on TikTok | 7NEWS.com.au,)   \n21                                       (Selena Gomez and Francia meet again in a TikTok video - Code List,)   \n22                                                          (TikTok star Hareem Shah's new video goes viral,)   \n23                                                          (TikTok star Hareem Shah's new video goes viral,)   \n24                                                          (TikTok star Hareem Shah's new video goes viral,)   \n25                                           (TikTok may let creators use their own product links in videos,)   \n26                                           (TikTok may let creators use their own product links in videos,)   \n27                                           (TikTok may let creators use their own product links in videos,)   \n28                                           (TikTok may let creators use their own product links in videos,)   \n29                                           (TikTok may let creators use their own product links in videos,)   \n\n                                                                                                                text_B  \\\n0                                                        (Coldplay to perform livestreamed set exclusively on TikTok,)   \n1                                     (Trump's attempt to ban TikTok in US dropped by Biden White House | wzzm13.com,)   \n2                                                         (Dutch group launches data harvesting claim against TikTok,)   \n3                                                            (Founder of TikTok's Chinese owner stepping down as CEO,)   \n4                                         (Meta, YouTube, and TikTok block Russian state media in Europe : worldnews,)   \n5                              (TikTok star\u2019s son killed in Alabama shooting, family confirms | KETK.com | FOX51.com,)   \n6                                          (Charge: Teen hit disabled teacher; apparent TikTok challenge - NewsBreak,)   \n7                                                (US drops Trump order targeting TikTok, plans its own review | WTOP,)   \n8                                   (North Carolina tattoo artist amasses large TikTok following | Biloxi Sun Herald,)   \n9                        (Florida Dollar General store manager says she was fired over viral TikToks | OurQuadCities,)   \n10                                                      (Netflix, TikTok block services in Russia to avoid crackdown,)   \n11  (What the heck is Pink Sauce and why should anyone not on TikTok care? Let us explain - Kimberley Daily Bulletin,)   \n12                                                                      (TikTok (\u30c6\u30a3\u30c3\u30af\u30c8\u30c3\u30af) \u3067 \u7dba\u9e97\u306a\u666f\u8272 (@tomo_pi) \u306e\u52d5\u753b\u3092\u898b\u308b ,)   \n13                                (FCC commissioner calls on Apple and Google to remove TikTok from their app stores,)   \n14                                     (How China's TikTok, Facebook influencers push propaganda | Daily Mail Online,)   \n15                                       (How China\u2019s TikTok, Facebook Influencers Push Propaganda \u2013 NBC Los Angeles,)   \n16           (\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WDVM25 & DCW50 | Washington, DC,)   \n17                     (Mum\u2019s stark warning over \u2018vile\u2019 graphic TikTok video being shared online | Hucknall Dispatch,)   \n18         (1 arrested in shooting that wounded TikTok star, killed teen \u2013 Boston News, Weather, Sports | WHDH 7News,)   \n19                                                                   (Idaho\u2019s TikTok Cop Has Been Fired - News Break,)   \n20                                                      (Okla. boy, 12, dies attempting TikTok challenge, police say,)   \n21                                (Tom Cruise is mistaken for a Japanese man for a viral video on TikTok - Code List,)   \n22                                                        (\"Pride cake\" TikTok Goes Viral Celebrating Mom's Allyship,)   \n23                                            ('Deepfake' Tom Cruise goes viral on TikTok with - One News Page VIDEO,)   \n24                                  (\u2018Healthy Coke\u2019 recipe baffles internet after TikTok video goes viral | WKBN.com,)   \n25                                                                                   (#\u5353\u5b9d\u5973\u795e Hashtag Videos on TikTok,)   \n26                                                                                 (#\u0e2d\u0e34\u0e42\u0e19\u0e23\u0e34 Hashtag Videos on TikTok,)   \n27                                                                      (#life_style_mekhma Hashtag Videos on TikTok,)   \n28                                                                        (#backgroundnoise Hashtag Videos on TikTok,)   \n29                                                                                 (#freesa Hashtag Videos on TikTok,)   \n\n   threshold_30 threshold_50 threshold_70  \n0     Duplicate    Duplicate    Duplicate  \n1     Duplicate    Duplicate    Duplicate  \n2     Duplicate    Duplicate    Duplicate  \n3     Duplicate    Duplicate    Duplicate  \n4     Duplicate    Duplicate    Duplicate  \n5     Duplicate    Duplicate    Duplicate  \n6     Duplicate    Duplicate    Duplicate  \n7     Duplicate    Duplicate    Duplicate  \n8     Duplicate    Duplicate    Duplicate  \n9     Duplicate    Duplicate    Duplicate  \n10      Non-Dup    Duplicate    Duplicate  \n11      Non-Dup    Duplicate    Duplicate  \n12      Non-Dup    Duplicate    Duplicate  \n13      Non-Dup    Duplicate    Duplicate  \n14      Non-Dup    Duplicate    Duplicate  \n15      Non-Dup    Duplicate    Duplicate  \n16      Non-Dup    Duplicate    Duplicate  \n17      Non-Dup    Duplicate    Duplicate  \n18      Non-Dup    Duplicate    Duplicate  \n19      Non-Dup    Duplicate    Duplicate  \n20      Non-Dup      Non-Dup    Duplicate  \n21      Non-Dup      Non-Dup    Duplicate  \n22      Non-Dup      Non-Dup    Duplicate  \n23      Non-Dup      Non-Dup    Duplicate  \n24      Non-Dup      Non-Dup    Duplicate  \n25      Non-Dup      Non-Dup    Duplicate  \n26      Non-Dup      Non-Dup    Duplicate  \n27      Non-Dup      Non-Dup    Duplicate  \n28      Non-Dup      Non-Dup    Duplicate  \n29      Non-Dup      Non-Dup    Duplicate  "}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}, {"name": "stderr", "output_type": "stream", "text": "22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_29 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_187 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_144 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_121_175 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_9 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_93 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_189 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_143 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_101 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_22 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_103 !\n22/11/18 00:52:54 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 49 for reason Container marked as failed: container_1668672144784_0019_01_000053 on host: hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:52:54 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 50 for reason Container marked as failed: container_1668672144784_0019_01_000054 on host: hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:52:54 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 49 on hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000053 on host: hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:52:54 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 50 on hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000054 on host: hub-msca-bdp-dphub-students-zhiliny-sw-kzgr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_6 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_157 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_155 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_90 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_26 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_13 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_109 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_191 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_121_126 !\n22/11/18 00:52:54 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_108 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_12 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_141 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_188 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_31 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_149 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_66 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_146 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_124 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_150 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_94 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_93 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_121_125 !\n22/11/18 00:53:05 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 45 on hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000049 on host: hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:53:05 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 45 for reason Container marked as failed: container_1668672144784_0019_01_000049 on host: hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:53:05 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 46 for reason Container marked as failed: container_1668672144784_0019_01_000050 on host: hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:53:05 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 46 on hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal: Container marked as failed: container_1668672144784_0019_01_000050 on host: hub-msca-bdp-dphub-students-zhiliny-sw-14cc.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_96 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_193 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_34 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_196 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_15 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_109 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_95 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_152 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_162_48 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_199 !\n22/11/18 00:53:05 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_225_154 !\n"}], "source": "pd.set_option('display.max_rows', 30)\ndups_combined_sample.toPandas()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### From the sample datafrmme above, we can see that jaccard distance = 0.5 is best here. \n#### Because when jaccard distance = 0.3, the model has large number of True negative error\n#### When jaccard distance = 0.7, the model has large number of False Positive error"}, {"cell_type": "markdown", "metadata": {}, "source": "### Build a histogram showing the overall distribution of \u201cnear-duplication\u201d"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nduptitle70= 76971\nduptitle50= 49379\nduptitle30= 41627\n\nxaxis = [0.3,0.5,0.7]\nplt.plot(xaxis,[duptitle30,duptitle50,duptitle70])\nplt.title('Overall New-Duplication Distribution')\nplt.xlabel('jaccard distance')\nplt.ylabel('New-Deplication Counts')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"data": {"text/plain": "'Tue, 26 January 2021 11:09:16'"}, "execution_count": 49, "metadata": {}, "output_type": "execute_result"}], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 4}